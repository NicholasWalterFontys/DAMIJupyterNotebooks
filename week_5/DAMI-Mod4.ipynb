{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-doing the Iris analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook I will re-do the Iris data analysis using a support vector machine as classifier. I will collect pieces and snippets from our [textbook](http://bit.ly/2xoFamE) and combine them in my own classification of the Iris data set using a support vector machine.\n",
    "\n",
    "I will setup the analysis using the generic Scikit-Learn steps as explained in the book. I will explain each step in a markdown cell, using my own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research question\n",
    "In this research I want to analyse data representing flower petals to classify their special type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "[_write a brief introduction to your analysis below. Briefly describe your classifier_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. load the iris data set (see Introducing Scikit-Learn)\n",
    "[_describe briefly what your data set is about and where you get the data from_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.4,  2.9,  4.3,  1.3],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.9,  3. ,  5.1,  1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], \n",
       "       dtype='<U10')}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# the loaded data is part of a learning library\n",
    "# it contains some test dataset describing flower petals which can be used to train an algorithm that should later\n",
    "# be able to classify new inputs and decide which specific type of flower it belongs to\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. create a feature matrix X and a target array y from the data set\n",
    "[_describe how you are going to prepare the data set and for what reason_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data:\n",
      "[[ 5.1  3.5]\n",
      " [ 4.9  3. ]\n",
      " [ 4.7  3.2]\n",
      " [ 4.6  3.1]\n",
      " [ 5.   3.6]\n",
      " [ 5.4  3.9]\n",
      " [ 4.6  3.4]\n",
      " [ 5.   3.4]\n",
      " [ 4.4  2.9]\n",
      " [ 4.9  3.1]\n",
      " [ 5.4  3.7]\n",
      " [ 4.8  3.4]\n",
      " [ 4.8  3. ]\n",
      " [ 4.3  3. ]\n",
      " [ 5.8  4. ]\n",
      " [ 5.7  4.4]\n",
      " [ 5.4  3.9]\n",
      " [ 5.1  3.5]\n",
      " [ 5.7  3.8]\n",
      " [ 5.1  3.8]\n",
      " [ 5.4  3.4]\n",
      " [ 5.1  3.7]\n",
      " [ 4.6  3.6]\n",
      " [ 5.1  3.3]\n",
      " [ 4.8  3.4]\n",
      " [ 5.   3. ]\n",
      " [ 5.   3.4]\n",
      " [ 5.2  3.5]\n",
      " [ 5.2  3.4]\n",
      " [ 4.7  3.2]\n",
      " [ 4.8  3.1]\n",
      " [ 5.4  3.4]\n",
      " [ 5.2  4.1]\n",
      " [ 5.5  4.2]\n",
      " [ 4.9  3.1]\n",
      " [ 5.   3.2]\n",
      " [ 5.5  3.5]\n",
      " [ 4.9  3.1]\n",
      " [ 4.4  3. ]\n",
      " [ 5.1  3.4]\n",
      " [ 5.   3.5]\n",
      " [ 4.5  2.3]\n",
      " [ 4.4  3.2]\n",
      " [ 5.   3.5]\n",
      " [ 5.1  3.8]\n",
      " [ 4.8  3. ]\n",
      " [ 5.1  3.8]\n",
      " [ 4.6  3.2]\n",
      " [ 5.3  3.7]\n",
      " [ 5.   3.3]\n",
      " [ 7.   3.2]\n",
      " [ 6.4  3.2]\n",
      " [ 6.9  3.1]\n",
      " [ 5.5  2.3]\n",
      " [ 6.5  2.8]\n",
      " [ 5.7  2.8]\n",
      " [ 6.3  3.3]\n",
      " [ 4.9  2.4]\n",
      " [ 6.6  2.9]\n",
      " [ 5.2  2.7]\n",
      " [ 5.   2. ]\n",
      " [ 5.9  3. ]\n",
      " [ 6.   2.2]\n",
      " [ 6.1  2.9]\n",
      " [ 5.6  2.9]\n",
      " [ 6.7  3.1]\n",
      " [ 5.6  3. ]\n",
      " [ 5.8  2.7]\n",
      " [ 6.2  2.2]\n",
      " [ 5.6  2.5]\n",
      " [ 5.9  3.2]\n",
      " [ 6.1  2.8]\n",
      " [ 6.3  2.5]\n",
      " [ 6.1  2.8]\n",
      " [ 6.4  2.9]\n",
      " [ 6.6  3. ]\n",
      " [ 6.8  2.8]\n",
      " [ 6.7  3. ]\n",
      " [ 6.   2.9]\n",
      " [ 5.7  2.6]\n",
      " [ 5.5  2.4]\n",
      " [ 5.5  2.4]\n",
      " [ 5.8  2.7]\n",
      " [ 6.   2.7]\n",
      " [ 5.4  3. ]\n",
      " [ 6.   3.4]\n",
      " [ 6.7  3.1]\n",
      " [ 6.3  2.3]\n",
      " [ 5.6  3. ]\n",
      " [ 5.5  2.5]\n",
      " [ 5.5  2.6]\n",
      " [ 6.1  3. ]\n",
      " [ 5.8  2.6]\n",
      " [ 5.   2.3]\n",
      " [ 5.6  2.7]\n",
      " [ 5.7  3. ]\n",
      " [ 5.7  2.9]\n",
      " [ 6.2  2.9]\n",
      " [ 5.1  2.5]\n",
      " [ 5.7  2.8]\n",
      " [ 6.3  3.3]\n",
      " [ 5.8  2.7]\n",
      " [ 7.1  3. ]\n",
      " [ 6.3  2.9]\n",
      " [ 6.5  3. ]\n",
      " [ 7.6  3. ]\n",
      " [ 4.9  2.5]\n",
      " [ 7.3  2.9]\n",
      " [ 6.7  2.5]\n",
      " [ 7.2  3.6]\n",
      " [ 6.5  3.2]\n",
      " [ 6.4  2.7]\n",
      " [ 6.8  3. ]\n",
      " [ 5.7  2.5]\n",
      " [ 5.8  2.8]\n",
      " [ 6.4  3.2]\n",
      " [ 6.5  3. ]\n",
      " [ 7.7  3.8]\n",
      " [ 7.7  2.6]\n",
      " [ 6.   2.2]\n",
      " [ 6.9  3.2]\n",
      " [ 5.6  2.8]\n",
      " [ 7.7  2.8]\n",
      " [ 6.3  2.7]\n",
      " [ 6.7  3.3]\n",
      " [ 7.2  3.2]\n",
      " [ 6.2  2.8]\n",
      " [ 6.1  3. ]\n",
      " [ 6.4  2.8]\n",
      " [ 7.2  3. ]\n",
      " [ 7.4  2.8]\n",
      " [ 7.9  3.8]\n",
      " [ 6.4  2.8]\n",
      " [ 6.3  2.8]\n",
      " [ 6.1  2.6]\n",
      " [ 7.7  3. ]\n",
      " [ 6.3  3.4]\n",
      " [ 6.4  3.1]\n",
      " [ 6.   3. ]\n",
      " [ 6.9  3.1]\n",
      " [ 6.7  3.1]\n",
      " [ 6.9  3.1]\n",
      " [ 5.8  2.7]\n",
      " [ 6.8  3.2]\n",
      " [ 6.7  3.3]\n",
      " [ 6.7  3. ]\n",
      " [ 6.3  2.5]\n",
      " [ 6.5  3. ]\n",
      " [ 6.2  3.4]\n",
      " [ 5.9  3. ]]\n",
      "\n",
      "expected output:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# getting feature matrix from set\n",
    "# this is an array of arrays describing sample data to identify the flower petals\n",
    "# originally it consists of 4 values for each line\n",
    "# i truncated each element to 2 elements to allow for easier visualisation later on\n",
    "# this means that we will only compare sepal length and width\n",
    "# while ignoring petal length and width\n",
    "\n",
    "X = np.array([i[:2] for i in iris[\"data\"]])\n",
    "print(\"input data:\")\n",
    "print(X)\n",
    "\n",
    "# creating target array\n",
    "# this is an array defining the type of the flower petal each item in the input list corresponds to\n",
    "# the algorithm uses this list to \"learn\" what characteristics belong to each type\n",
    "Y = np.array(iris[\"target\"])\n",
    "print(\"\\nexpected output:\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. split the data in a test and train set\n",
    "[_describe how you are going to split the data set and for what reason_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 2)\n",
      "(5, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVIqAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faad25925c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# because I personally do not know anything about flower petals, I have to use some of the provided data to test\n",
    "# the results of the learning algorithms\n",
    "# i split of 5 items to use to test, the rest (145 items) will be used to train the algorithm\n",
    "\n",
    "# split input data\n",
    "X_learn = X[:-5]\n",
    "print(X_learn.shape)    # make sure that data has the correct shape (145 samples with two features)\n",
    "X_test = X[-5:]\n",
    "print(X_test.shape)     # make sure that data has the correct shape (5 samples with two features)\n",
    "\n",
    "# split expected data\n",
    "Y_learn = Y[:-5]\n",
    "Y_test = Y[-5:]\n",
    "\n",
    "# importing libraries to visualize dataset as a first look\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# rotating data to retrieve 3 sets of 150 items each instead of 150 sets of 3 items each\n",
    "# this is neccessary because it is the format expected by matplotlib\n",
    "input90 = np.rot90(X)\n",
    "\n",
    "# initialise plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add data to plot and show\n",
    "ax.scatter(input90[0], input90[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. fit the model using the train set, applying the support vector machine\n",
    "[_describe how you are going to fit the model and what model parameters you are using_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. evaluate the model by predicting the test set\n",
    "[_describe how you are going to evaluate the results of your classifier and what it means_]\n",
    "both in the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kernel type: linear\n",
      "Prediction for test data: \n",
      "[2 1 2 2 1]\n",
      "Expected result:\n",
      "[2 2 2 2 2]\n",
      "Percentage: 60.0\n",
      "\n",
      "\n",
      "Testing kernel type: rbf\n",
      "Prediction for test data: \n",
      "[2 1 2 2 1]\n",
      "Expected result:\n",
      "[2 2 2 2 2]\n",
      "Percentage: 60.0\n",
      "\n",
      "\n",
      "Testing kernel type: poly\n",
      "Prediction for test data: \n",
      "[2 1 2 1 1]\n",
      "Expected result:\n",
      "[2 2 2 2 2]\n",
      "Percentage: 40.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from scipy import spatial\n",
    "\n",
    "def get_matching_percentage(list_1, list_2):\n",
    "    if not len(list_1) == len(list_2):\n",
    "        print(\"list lengths not matching\")\n",
    "        return -1\n",
    "    count = 0;\n",
    "    correct_count = 0;\n",
    "    for i in range(len(list_1)):\n",
    "        count +=1\n",
    "        if(list_1[i] == list_2[i]):\n",
    "            correct_count +=1\n",
    "    return 100 * (correct_count / count)\n",
    "        \n",
    "        \n",
    "model_types = [\"linear\", \"rbf\", \"poly\"]\n",
    "\n",
    "models = []\n",
    "for mt in model_types:\n",
    "    print(\"Testing kernel type: \" + mt)\n",
    "    m = SVC(kernel=mt, C=1)\n",
    "    m.fit(X_learn, Y_learn)\n",
    "    \n",
    "    print(\"Prediction for test data: \")\n",
    "    prediction = m.predict(X_test)\n",
    "    print(prediction)\n",
    "    \n",
    "    print(\"Expected result:\")\n",
    "    print(Y_test)\n",
    "    \n",
    "    perc = get_matching_percentage(prediction, Y_test)\n",
    "    #perc = 100 * (1 - spatial.distance.cosine(prediction, Y_test)) \n",
    "    print(\"Percentage: {}\".format(perc))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of result\n",
    "[_ reflect on the accuracy of your result and whether you have properly answered your research question _]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
